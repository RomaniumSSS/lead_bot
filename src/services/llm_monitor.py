"""ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ LLM API."""

from datetime import UTC, datetime, timedelta

from anthropic.types import Usage

from src.database.models import Lead, LLMUsage
from src.utils.logger import logger

# AICODE-NOTE: Ð¢Ð°Ñ€Ð¸Ñ„Ñ‹ Claude (Ð² USD Ð·Ð° 1M Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²)
# ÐÐºÑ‚ÑƒÐ°Ð»ÑŒÐ½Ð¾ Ð½Ð° Ð´ÐµÐºÐ°Ð±Ñ€ÑŒ 2024. Ð•ÑÐ»Ð¸ Ñ‚Ð°Ñ€Ð¸Ñ„Ñ‹ Ð¸Ð·Ð¼ÐµÐ½ÑÑ‚ÑÑ â€” Ð¾Ð±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð·Ð´ÐµÑÑŒ.
# Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº: https://www.anthropic.com/pricing
PRICING = {
    "claude-sonnet-4-20250514": {
        "input": 3.00,
        "output": 15.00,
        "cache_write": 3.75,
        "cache_read": 0.30,
    },
    "claude-3-5-haiku-20241022": {
        "input": 1.00,
        "output": 5.00,
        "cache_write": 1.25,
        "cache_read": 0.10,
    },
}


async def track_llm_usage(
    model: str,
    usage: Usage,
    request_type: str,
    lead: Lead | None = None,
) -> None:
    """
    Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ LLM Ð² Ð‘Ð”.

    Args:
        model: ÐœÐ¾Ð´ÐµÐ»ÑŒ Claude (Ð½Ð°Ð¿Ñ€. "claude-sonnet-4-20250514")
        usage: ÐžÐ±ÑŠÐµÐºÑ‚ Usage Ð¾Ñ‚ Claude API
        request_type: Ð¢Ð¸Ð¿ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° (greeting, free_chat, suggested_questions, etc.)
        lead: ÐžÐ±ÑŠÐµÐºÑ‚ Ð»Ð¸Ð´Ð° (ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ)
    """
    # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ‚Ð°Ñ€Ð¸Ñ„Ñ‹ Ð´Ð»Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸ (Ð¸Ð»Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ñ‚Ð°Ñ€Ð¸Ñ„Ñ‹ Sonnet Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ)
    pricing = PRICING.get(model, PRICING["claude-sonnet-4-20250514"])

    # Ð¡Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼ ÑÑ‚Ð¾Ð¸Ð¼Ð¾ÑÑ‚ÑŒ (Ð² Ñ†ÐµÐ½Ñ‚Ð°Ñ…)
    cost_input = int((usage.input_tokens / 1_000_000) * pricing["input"] * 100)
    cost_output = int((usage.output_tokens / 1_000_000) * pricing["output"] * 100)

    # AICODE-NOTE: cache_creation_input_tokens Ð¸ cache_read_input_tokens Ð¼Ð¾Ð³ÑƒÑ‚ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²Ð¾Ð²Ð°Ñ‚ÑŒ
    # ÐµÑÐ»Ð¸ prompt caching Ð½Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð»ÑÑ
    cache_creation = getattr(usage, "cache_creation_input_tokens", 0)
    cache_read = getattr(usage, "cache_read_input_tokens", 0)

    cost_cache_creation = int((cache_creation / 1_000_000) * pricing["cache_write"] * 100)
    cost_cache_read = int((cache_read / 1_000_000) * pricing["cache_read"] * 100)

    total_cost = cost_input + cost_output + cost_cache_creation + cost_cache_read

    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð² Ð‘Ð”
    try:
        await LLMUsage.create(
            lead=lead,
            model=model,
            request_type=request_type,
            input_tokens=usage.input_tokens,
            output_tokens=usage.output_tokens,
            cache_creation_tokens=cache_creation,
            cache_read_tokens=cache_read,
            cost_input=cost_input,
            cost_output=cost_output,
            cost_cache_creation=cost_cache_creation,
            cost_cache_read=cost_cache_read,
            total_cost=total_cost,
        )

        # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ ÑÑ‚Ð¾Ð¸Ð¼Ð¾ÑÑ‚ÑŒ > 0 (Ð´Ð»Ñ ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸Ð¸ Ð¼ÐµÑÑ‚Ð° Ð² Ð»Ð¾Ð³Ð°Ñ…)
        if total_cost > 0:
            logger.info(
                f"ðŸ’° LLM Usage: {request_type} | "
                f"tokens={usage.input_tokens + usage.output_tokens} | "
                f"cost=${total_cost / 100:.4f} | "
                f"cache_hit={cache_read > 0}"
            )
    except Exception as e:
        # AICODE-NOTE: ÐÐµ Ð¿Ð°Ð´Ð°ÐµÐ¼ ÐµÑÐ»Ð¸ Ð½Ðµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑÐ¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ LLM usage: {e}", exc_info=True)


async def get_daily_stats() -> dict[str, int | float]:
    """
    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ LLM Ð·Ð° ÑÐµÐ³Ð¾Ð´Ð½Ñ.

    Returns:
        dict ÑÐ¾ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¾Ð¹:
            - total_requests: ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²
            - input_tokens: ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ input Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²
            - output_tokens: ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ output Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²
            - cache_hit_rate: Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚ ÐºÑÑˆ-Ñ…Ð¸Ñ‚Ð¾Ð²
            - total_cost: Ð¾Ð±Ñ‰Ð°Ñ ÑÑ‚Ð¾Ð¸Ð¼Ð¾ÑÑ‚ÑŒ Ð² Ñ†ÐµÐ½Ñ‚Ð°Ñ…
            - sonnet_requests: ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² Ðº Sonnet
            - sonnet_cost: ÑÑ‚Ð¾Ð¸Ð¼Ð¾ÑÑ‚ÑŒ Sonnet Ð² Ñ†ÐµÐ½Ñ‚Ð°Ñ…
            - haiku_requests: ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² Ðº Haiku
            - haiku_cost: ÑÑ‚Ð¾Ð¸Ð¼Ð¾ÑÑ‚ÑŒ Haiku Ð² Ñ†ÐµÐ½Ñ‚Ð°Ñ…
    """
    # Ð¢ÐµÐºÑƒÑ‰Ð°Ñ Ð´Ð°Ñ‚Ð° (Ð½Ð°Ñ‡Ð°Ð»Ð¾ Ð´Ð½Ñ Ð¿Ð¾ UTC)
    today_start = datetime.now(tz=UTC).replace(hour=0, minute=0, second=0, microsecond=0)

    # Ð’ÑÐµ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð·Ð° ÑÐµÐ³Ð¾Ð´Ð½Ñ
    usage_records = await LLMUsage.filter(created_at__gte=today_start).all()

    if not usage_records:
        return {
            "total_requests": 0,
            "input_tokens": 0,
            "output_tokens": 0,
            "cache_hit_rate": 0.0,
            "total_cost": 0,
            "sonnet_requests": 0,
            "sonnet_cost": 0,
            "haiku_requests": 0,
            "haiku_cost": 0,
        }

    # ÐÐ³Ñ€ÐµÐ³Ð°Ñ†Ð¸Ñ
    total_requests = len(usage_records)
    input_tokens = sum(r.input_tokens for r in usage_records)
    output_tokens = sum(r.output_tokens for r in usage_records)
    total_cost = sum(r.total_cost for r in usage_records)

    # Cache hit rate
    cache_read_tokens = sum(r.cache_read_tokens for r in usage_records)
    total_input_tokens = input_tokens + cache_read_tokens
    cache_hit_rate = (
        (cache_read_tokens / total_input_tokens * 100) if total_input_tokens > 0 else 0.0
    )

    # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¿Ð¾ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼
    sonnet_records = [r for r in usage_records if "sonnet" in r.model.lower()]
    haiku_records = [r for r in usage_records if "haiku" in r.model.lower()]

    sonnet_requests = len(sonnet_records)
    sonnet_cost = sum(r.total_cost for r in sonnet_records)

    haiku_requests = len(haiku_records)
    haiku_cost = sum(r.total_cost for r in haiku_records)

    return {
        "total_requests": total_requests,
        "input_tokens": input_tokens,
        "output_tokens": output_tokens,
        "cache_hit_rate": cache_hit_rate,
        "total_cost": total_cost,
        "sonnet_requests": sonnet_requests,
        "sonnet_cost": sonnet_cost,
        "haiku_requests": haiku_requests,
        "haiku_cost": haiku_cost,
    }


async def get_lead_stats(lead: Lead) -> dict[str, int | float]:
    """
    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ LLM Ð¿Ð¾ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð¼Ñƒ Ð»Ð¸Ð´Ñƒ.

    Args:
        lead: ÐžÐ±ÑŠÐµÐºÑ‚ Ð»Ð¸Ð´Ð°

    Returns:
        dict ÑÐ¾ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¾Ð¹:
            - total_requests: ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²
            - input_tokens: ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ input Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²
            - output_tokens: ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ output Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²
            - total_cost: Ð¾Ð±Ñ‰Ð°Ñ ÑÑ‚Ð¾Ð¸Ð¼Ð¾ÑÑ‚ÑŒ Ð² Ñ†ÐµÐ½Ñ‚Ð°Ñ…
    """
    usage_records = await LLMUsage.filter(lead=lead).all()

    if not usage_records:
        return {
            "total_requests": 0,
            "input_tokens": 0,
            "output_tokens": 0,
            "total_cost": 0,
        }

    total_requests = len(usage_records)
    input_tokens = sum(r.input_tokens for r in usage_records)
    output_tokens = sum(r.output_tokens for r in usage_records)
    total_cost = sum(r.total_cost for r in usage_records)

    return {
        "total_requests": total_requests,
        "input_tokens": input_tokens,
        "output_tokens": output_tokens,
        "total_cost": total_cost,
    }


async def get_weekly_stats() -> dict[str, int | float]:
    """
    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ LLM Ð·Ð° Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ 7 Ð´Ð½ÐµÐ¹.

    Returns:
        dict ÑÐ¾ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¾Ð¹ (Ð°Ð½Ð°Ð»Ð¾Ð³Ð¸Ñ‡Ð½Ð¾ get_daily_stats)
    """
    week_start = datetime.now(tz=UTC) - timedelta(days=7)

    usage_records = await LLMUsage.filter(created_at__gte=week_start).all()

    if not usage_records:
        return {
            "total_requests": 0,
            "input_tokens": 0,
            "output_tokens": 0,
            "cache_hit_rate": 0.0,
            "total_cost": 0,
            "sonnet_requests": 0,
            "sonnet_cost": 0,
            "haiku_requests": 0,
            "haiku_cost": 0,
        }

    total_requests = len(usage_records)
    input_tokens = sum(r.input_tokens for r in usage_records)
    output_tokens = sum(r.output_tokens for r in usage_records)
    total_cost = sum(r.total_cost for r in usage_records)

    cache_read_tokens = sum(r.cache_read_tokens for r in usage_records)
    total_input_tokens = input_tokens + cache_read_tokens
    cache_hit_rate = (
        (cache_read_tokens / total_input_tokens * 100) if total_input_tokens > 0 else 0.0
    )

    sonnet_records = [r for r in usage_records if "sonnet" in r.model.lower()]
    haiku_records = [r for r in usage_records if "haiku" in r.model.lower()]

    sonnet_requests = len(sonnet_records)
    sonnet_cost = sum(r.total_cost for r in sonnet_records)

    haiku_requests = len(haiku_records)
    haiku_cost = sum(r.total_cost for r in haiku_records)

    return {
        "total_requests": total_requests,
        "input_tokens": input_tokens,
        "output_tokens": output_tokens,
        "cache_hit_rate": cache_hit_rate,
        "total_cost": total_cost,
        "sonnet_requests": sonnet_requests,
        "sonnet_cost": sonnet_cost,
        "haiku_requests": haiku_requests,
        "haiku_cost": haiku_cost,
    }
